{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ffbc147",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "import sys\n",
    "\n",
    "\n",
    "class nconv(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(nconv,self).__init__()\n",
    "\n",
    "    def forward(self,x, A):\n",
    "        x = torch.einsum('ncvl,vw->ncwl',(x,A))\n",
    "        return x.contiguous()\n",
    "\n",
    "class linear(nn.Module):\n",
    "    def __init__(self,c_in,c_out):\n",
    "        super(linear,self).__init__()\n",
    "        self.mlp = torch.nn.Conv2d(c_in, c_out, kernel_size=(1, 1), padding=(0,0), stride=(1,1), bias=True) # \n",
    "\n",
    "    def forward(self,x):\n",
    "        return self.mlp(x)\n",
    "\n",
    "class gcn(nn.Module):\n",
    "    def __init__(self,c_in,c_out,dropout,support_len=3,order=2):\n",
    "        super(gcn,self).__init__()\n",
    "        self.nconv = nconv()\n",
    "        c_in = (order*support_len+1)*c_in\n",
    "        self.mlp = linear(c_in,c_out)\n",
    "        self.dropout = dropout\n",
    "        self.order = order\n",
    "\n",
    "    def forward(self,x,support):\n",
    "        out = [x]\n",
    "        for a in support:\n",
    "            x1 = self.nconv(x,a)\n",
    "            out.append(x1)\n",
    "            for k in range(2, self.order + 1):\n",
    "#                 print (\"K\")\n",
    "#                 print (k)\n",
    "                \n",
    "                \n",
    "#                 print (x1.shape)\n",
    "#                 print (a.shape)\n",
    "                \n",
    "                x2 = self.nconv(x1,a)\n",
    "                out.append(x2)\n",
    "                x1 = x2\n",
    "        \n",
    "        h = torch.cat(out,dim=1)\n",
    "        h = self.mlp(h)\n",
    "        h = F.dropout(h, self.dropout, training=self.training)\n",
    "        return h\n",
    "\n",
    "\n",
    "class gwnet(nn.Module):\n",
    "    def __init__(self, device, num_nodes, dropout=0.3, supports=None, gcn_bool=True, addaptadj=True, aptinit=None, in_dim=2,out_dim=12,residual_channels=32,dilation_channels=32,skip_channels=256,end_channels=512,kernel_size=2,blocks=4,layers=2):\n",
    "        super(gwnet, self).__init__()\n",
    "        self.dropout = dropout\n",
    "        self.blocks = blocks\n",
    "        self.layers = layers\n",
    "        self.gcn_bool = gcn_bool\n",
    "        self.addaptadj = addaptadj\n",
    "\n",
    "        self.filter_convs = nn.ModuleList()\n",
    "        self.gate_convs = nn.ModuleList()\n",
    "        self.residual_convs = nn.ModuleList()\n",
    "        self.skip_convs = nn.ModuleList()\n",
    "        self.bn = nn.ModuleList()\n",
    "        self.gconv = nn.ModuleList()\n",
    "\n",
    "        self.start_conv = nn.Conv2d(in_channels=in_dim,\n",
    "                                    out_channels=residual_channels,\n",
    "                                    kernel_size=(1,1))\n",
    "        self.supports = supports\n",
    "\n",
    "        receptive_field = 1\n",
    "\n",
    "        self.supports_len = 0\n",
    "        if supports is not None:\n",
    "            self.supports_len += len(supports)\n",
    "\n",
    "        if gcn_bool and addaptadj:\n",
    "            if aptinit is None:\n",
    "                if supports is None:\n",
    "                    self.supports = []\n",
    "                self.nodevec1 = nn.Parameter(torch.randn(num_nodes, 10).to(device), requires_grad=True).to(device)\n",
    "                self.nodevec2 = nn.Parameter(torch.randn(10, num_nodes).to(device), requires_grad=True).to(device)\n",
    "                self.supports_len +=1\n",
    "            else:\n",
    "                if supports is None:\n",
    "                    self.supports = []\n",
    "                m, p, n = torch.svd(aptinit)\n",
    "                initemb1 = torch.mm(m[:, :10], torch.diag(p[:10] ** 0.5))\n",
    "                initemb2 = torch.mm(torch.diag(p[:10] ** 0.5), n[:, :10].t())\n",
    "                self.nodevec1 = nn.Parameter(initemb1, requires_grad=True).to(device)\n",
    "                self.nodevec2 = nn.Parameter(initemb2, requires_grad=True).to(device)\n",
    "                self.supports_len += 1\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        for b in range(blocks):\n",
    "            additional_scope = kernel_size - 1\n",
    "            new_dilation = 1\n",
    "            for i in range(layers):\n",
    "                # dilated convolutions\n",
    "                self.filter_convs.append(nn.Conv2d(in_channels=residual_channels,\n",
    "                                                   out_channels=dilation_channels,\n",
    "                                                   kernel_size=(1,kernel_size),dilation=new_dilation))\n",
    "\n",
    "                self.gate_convs.append(nn.Conv1d(in_channels=residual_channels,\n",
    "                                                 out_channels=dilation_channels,\n",
    "                                                 kernel_size=(1, kernel_size), dilation=new_dilation))\n",
    "\n",
    "                # 1x1 convolution for residual connection\n",
    "                self.residual_convs.append(nn.Conv1d(in_channels=dilation_channels,\n",
    "                                                     out_channels=residual_channels,\n",
    "                                                     kernel_size=(1, 1)))\n",
    "\n",
    "                # 1x1 convolution for skip connection\n",
    "                self.skip_convs.append(nn.Conv1d(in_channels=dilation_channels,\n",
    "                                                 out_channels=skip_channels,\n",
    "                                                 kernel_size=(1, 1)))\n",
    "                self.bn.append(nn.BatchNorm2d(residual_channels))\n",
    "                new_dilation *=2\n",
    "                \n",
    "                \n",
    "#                 print ('dilation: ', new_dilation)\n",
    "                \n",
    "                \n",
    "                receptive_field += additional_scope\n",
    "                additional_scope *= 2\n",
    "                if self.gcn_bool:\n",
    "                    self.gconv.append(gcn(dilation_channels,residual_channels,dropout,support_len=self.supports_len))\n",
    "\n",
    "\n",
    "\n",
    "        self.end_conv_1 = nn.Conv2d(in_channels=skip_channels,\n",
    "                                  out_channels=end_channels,\n",
    "                                  kernel_size=(1,1),\n",
    "                                  bias=True)\n",
    "\n",
    "        self.end_conv_2 = nn.Conv2d(in_channels=end_channels,\n",
    "                                    out_channels=out_dim,\n",
    "                                    kernel_size=(1,1),\n",
    "                                    bias=True)\n",
    "\n",
    "        self.receptive_field = receptive_field\n",
    "\n",
    "\n",
    "\n",
    "    def forward(self, input):\n",
    "        in_len = input.size(3)\n",
    "        if in_len<self.receptive_field:\n",
    "            x = nn.functional.pad(input,(self.receptive_field-in_len,0,0,0))\n",
    "        else:\n",
    "            x = input\n",
    "        x = self.start_conv(x)\n",
    "        skip = 0\n",
    "\n",
    "        # calculate the current adaptive adj matrix once per iteration\n",
    "        new_supports = None\n",
    "        if self.gcn_bool and self.addaptadj and self.supports is not None:\n",
    "            adp = F.softmax(F.relu(torch.mm(self.nodevec1, self.nodevec2)), dim=1)\n",
    "            new_supports = self.supports + [adp]\n",
    "\n",
    "        # WaveNet layers\n",
    "        for i in range(self.blocks * self.layers):\n",
    "#             print ('layers: ', self.blocks * self.layers)\n",
    "\n",
    "            #            |----------------------------------------|     *residual*\n",
    "            #            |                                        |\n",
    "            #            |    |-- conv -- tanh --|                |\n",
    "            # -> dilate -|----|                  * ----|-- 1x1 -- + -->\t*input*\n",
    "            #                 |-- conv -- sigm --|     |\n",
    "            #                                         1x1\n",
    "            #                                          |\n",
    "            # ---------------------------------------> + ------------->\t*skip*\n",
    "\n",
    "            #(dilation, init_dilation) = self.dilations[i]\n",
    "\n",
    "            #residual = dilation_func(x, dilation, init_dilation, i)\n",
    "            residual = x\n",
    "            # dilated convolution\n",
    "            filter = self.filter_convs[i](residual)\n",
    "            filter = torch.tanh(filter)\n",
    "            gate = self.gate_convs[i](residual)\n",
    "            gate = torch.sigmoid(gate)\n",
    "            x = filter * gate\n",
    "\n",
    "            # parametrized skip connection\n",
    "\n",
    "            s = x\n",
    "            s = self.skip_convs[i](s)\n",
    "            try:\n",
    "                skip = skip[:, :, :,  -s.size(3):]\n",
    "            except:\n",
    "                skip = 0\n",
    "            skip = s + skip\n",
    "\n",
    "\n",
    "            if self.gcn_bool and self.supports is not None:\n",
    "                if self.addaptadj:\n",
    "                    x = self.gconv[i](x, new_supports)\n",
    "                else:\n",
    "                    x = self.gconv[i](x,self.supports)\n",
    "            else:\n",
    "                x = self.residual_convs[i](x)\n",
    "\n",
    "            x = x + residual[:, :, :, -x.size(3):]\n",
    "\n",
    "\n",
    "            x = self.bn[i](x)\n",
    "\n",
    "        x = F.relu(skip)\n",
    "        x = F.relu(self.end_conv_1(x))\n",
    "        x = self.end_conv_2(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "QRFT",
   "language": "python",
   "name": "qrft"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
